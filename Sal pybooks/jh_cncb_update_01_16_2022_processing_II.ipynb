# %% markdown
# ### Compute variant associated IR and FR (normalization: v4 with lags)
# ### Final step
# ### Update: from 12/15 to 1/16/22
#
# ### Import IR, PP and lagged_FR - assemble them into a single file each.
# %% codecell
import pandas as pd
import numpy as np
import os
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from datetime import datetime as dt
from datetime import timedelta
from scipy.signal import lfilter
import pickle as pickle
import sys
import gc
# %% codecell
# load required files - lagged FR
dates = pd.read_table('dates.txt',header=None,index_col=0)
old_date = dates.loc['old_date'].values[0]
new_date = dates.loc['new_date'].values[0]

# LOAD ALL OUTPUTS YOU GENERATED

with open(f"../Input_files/lagged_FR1_{new_date.replace('_','')}.pkl",'rb') as file:
    lagged_FR1 = pickle.load(file)

with open(f"../Input_files/lagged_FR2_{new_date.replace('_','')}.pkl",'rb') as file:
    lagged_FR2 = pickle.load(file)

# %% codecell
list_dates=list(lagged_FR1.keys())

lagged_FR=lagged_FR1

for date in tqdm(list_dates[700:717]):

    for var in lagged_FR[date].keys():
        lagged_FR[date][var]=lagged_FR2[date][var]

# If you have more than 2 partial files, keep adding them to the aggregate list - for example, supposing you have two more partial files lagged_FR3 and lagged_FR4:

# for date in tqdm(list_dates[717:730]):   # CHANGE DATES
#    for var in lagged_FR[date].keys():\n",
#        lagged_FR[date][var]=lagged_FR3[date][var]
#
# for date in tqdm(list_dates[730:745]):  # CHANGE DATES
#     for var in lagged_FR[date].keys():\n",
#         lagged_FR[date][var]=lagged_FR4[date][var]

del lagged_FR1
del lagged_FR2 # delete all partial files
gc.collect()
# %% codecell
# IR

# LOAD ALL OUTPUTS YOU GENERATED

with open(f"../Input_files/IR1_{new_date.replace('_','')}.pkl",'rb') as file:
    IR1 = pickle.load(file)

with open(f"../Input_files/IR2_{new_date.replace('_','')}.pkl",'rb') as file:
    IR2 = pickle.load(file)

# %% codecell
IR=IR1

for date in tqdm(list_dates[700:717]):
    for var in IR[date].keys():
        IR[date][var]=IR2[date][var]

# SAME AS ABOVE - EXTEND TO ALL PARTIAL FILES AS NEEDED

del IR1
del IR2
gc.collect()
# %% codecell
# PP

# LOAD ALL OUTPUTS YOU GENERATED

with open(f"../Input_files/PP1_{new_date.replace('_','')}.pkl",'rb') as file:
    PP1 = pickle.load(file)

with open(f"../Input_files/PP2_{new_date.replace('_','')}.pkl",'rb') as file:
    PP2 = pickle.load(file)


del PP1
del PP2

gc.collect()
# %% codecell
PP=PP1

# SAME AS ABOVE - EXTEND TO ALL PARTIAL FILES AS NEEDED

for date in tqdm(list_dates[700:717]):
    for var in PP[date].keys():
        PP[date][var]=PP2[date][var]

# %% codecell
# save

with open(f"../Input_files/lagged_FR_{old_date}_{new_date}.pkl",'wb') as file:
    pickle.dump(lagged_FR,file,protocol=pickle.HIGHEST_PROTOCOL)
with open(f"../Input_files/IR_{old_date}_{new_date}.pkl",'wb') as file:
    pickle.dump(IR,file,protocol=pickle.HIGHEST_PROTOCOL)
with open(f"../Input_files/PP_{old_date}_{new_date}.pkl",'wb') as file:
    pickle.dump(PP,file,protocol=pickle.HIGHEST_PROTOCOL)

# %% codecell
# If you just need to load the files before for the final table writing step, uncomment this cell and execute

#with open(f"../Input_files/lagged_FR_{old_date}_{new_date}.pkl",'rb') as file:
#    lagged_FR = pickle.load(file)

#with open(f"../Input_files/IR_{old_date}_{new_date}.pkl",'rb') as file:
#    IR = pickle.load(file)

#with open(f"../Input_files/PP_{old_date}_{new_date}.pkl",'rb') as file:
#    PP = pickle.load(file)

#list_dates=list(lagged_FR.keys())
# %% codecell
with open(f"../Input_files/aggregated_mutations_window00_{dt.strftime(dt.strptime(old_date,'%m_%d_%y') + timedelta(days=1),'%m_%d_%y')}_{new_date}.pkl",'rb') as file:
    aggregated_mutations_window = pickle.load(file)
# %% codecell
# Finally, assemble and save the output tables for GPR.

# CHANGE INDEXES - these will be the same as in jh_cncb_update_processing_I.py, line 58 (AFTER YOU UPDATED THE INDEXES TO CURRENT UPDATE)
for date in tqdm(list_dates[684:717]):
    unique_muts = pd.DataFrame(list(aggregated_mutations_window[date].keys()),columns=['descriptor'])
    unique_muts = pd.DataFrame.join(unique_muts,pd.DataFrame(unique_muts['descriptor'].str.split(',').to_list())) #assign columns with parsed descriptor
    unique_muts.set_index('descriptor',inplace=True)
    unique_muts[0] = pd.to_numeric(unique_muts[0])
    unique_muts[1] = pd.to_numeric(unique_muts[1])
    unique_muts.sort_values([0,1],inplace=True)
    unique_muts.columns = ['Start','End','Ref','Alt','VEP','Variant Type','Ref_AA','Alt_AA','AA']

    counts = {}
    num_countries = {}
    infection_rate = {}
    fatality_rate = {}
    counted_countries = {}

    for desc in unique_muts.index:
        counts[desc] = len(aggregated_mutations_window[date][desc])
        num_countries[desc] = len(set(aggregated_mutations_window[date][desc]))
        ir_sum=sum(IR[date][desc])
        pp_sum=sum(PP[date][desc])
        if pp_sum==0:
            infection_rate[desc] = ir_sum
        else:
            infection_rate[desc] = ir_sum/pp_sum #/counts[desc]
        #infection_rate[desc] = np.mean(aggregated_mutations_window[date][desc][1]) #since accumulated over countries before denomenator of mean is wrong
        if counts[desc]==0:
            fatality_rate[desc] = sum(lagged_FR[date][desc])
        else:
            fatality_rate[desc] = sum(lagged_FR[date][desc])/counts[desc]#/counts[desc]
        #fatality_rate[desc] = np.mean(aggregated_mutations_window[date][desc][2]) #since accumulated over countries before denomenator of mean is wrong
        counted_countries[desc] = dict(Counter(aggregated_mutations_window[date][desc]))

    unique_muts['counts'] = unique_muts.index.to_series().map(counts)
    unique_muts['countries'] = unique_muts.index.to_series().map(num_countries)
    unique_muts['infection_rate'] = unique_muts.index.to_series().map(infection_rate)
    unique_muts['fatality_rate'] = unique_muts.index.to_series().map(fatality_rate)
    unique_muts['counted_countries'] = unique_muts.index.to_series().map(counted_countries)
    unique_muts.index = unique_muts.index.str.split(',').str[0:4].str.join('_')
    unique_muts.sort_values('counts',ascending=False)
    # CREATE A FOLDER FOR THE OUTPUT IN ../Output Files: cumulative_daily_AF_v4_lag_<your date>
    # CHANGE FOLDER NAME BELOW TO THE FOLDER YOU CREATED
    unique_muts.to_csv(f"../Output Files/cumulative_daily_AF_v4_lag_{new_date}/{date.strftime('%m_%d_%y')}.csv")
# %% codecell
